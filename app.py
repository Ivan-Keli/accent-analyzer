import streamlit as st
import os
import tempfile
import urllib.request
import urllib.parse
import requests
import re
import subprocess
import numpy as np
import librosa
import torch
from pydub import AudioSegment
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
import yt_dlp
import json

# Set page title and configuration
st.set_page_config(page_title="Accent Analyzer", layout="wide")
st.title("üé§ Accent Analyzer: Video to Accent Detection")
st.markdown("Upload a video URL to analyze the speaker's accent using advanced audio processing")

# Initialize session state for tracking analysis progress
if 'analyzed' not in st.session_state:
    st.session_state['analyzed'] = False
if 'results' not in st.session_state:
    st.session_state['results'] = None

# Enhanced device detection using torch
@st.cache_resource
def get_processing_device():
    """Get the best available device for audio processing"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        st.sidebar.success(f"üöÄ GPU acceleration available: {torch.cuda.get_device_name()}")
        return device
    else:
        device = torch.device('cpu')
        st.sidebar.info("üíª Using CPU for processing")
        return device

# Initialize device
DEVICE = get_processing_device()

# Function to check if URL is valid
def is_valid_url(url):
    try:
        result = urllib.parse.urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

# Function to check if FFmpeg is available
def check_ffmpeg():
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

# Enhanced video download with better format handling
def download_video(url):
    temp_dir = tempfile.mkdtemp()
    
    # Check if it's a YouTube URL
    if "youtube.com" in url or "youtu.be" in url:
        try:
            # Try multiple format strategies for better compatibility
            format_strategies = [
                # Strategy 1: Best quality with audio, prefer mp4
                'best[height<=720][acodec!=none][ext=mp4]/best[acodec!=none][ext=mp4]',
                # Strategy 2: Any format with audio, prefer lower quality for compatibility  
                'worst[acodec!=none]/best[acodec!=none]',
                # Strategy 3: Merge video and audio if separate
                'bestvideo[height<=720]+bestaudio[ext=m4a]/best[height<=720]',
                # Strategy 4: Last resort - any available format
                'best/worst'
            ]
            
            for i, format_selector in enumerate(format_strategies):
                try:
                    st.info(f"üîÑ Trying download strategy {i+1}/4...")
                    
                    ydl_opts = {
                        'format': format_selector,
                        'outtmpl': os.path.join(temp_dir, 'video.%(ext)s'),
                        'quiet': True,
                        'no_warnings': True,
                        'merge_output_format': 'mp4',
                        'prefer_ffmpeg': True,
                        'writeinfojson': False,
                        'writedescription': False,
                        'writesubtitles': False,
                    }
                    
                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        # Get video info first
                        info = ydl.extract_info(url, download=False)
                        
                        # Check video duration (skip very long videos)
                        duration = info.get('duration', 0)
                        if duration > 3600:  # 1 hour limit
                            st.warning(f"‚ö†Ô∏è Video is {duration//60} minutes long. Processing may take a while.")
                            if duration > 7200:  # 2 hour hard limit
                                st.error("‚ùå Video is too long (over 2 hours). Please use a shorter video.")
                                return None
                        
                        # Enhanced audio detection
                        has_audio = False
                        audio_info = []
                        
                        if 'formats' in info:
                            for fmt in info['formats']:
                                if fmt.get('acodec') and fmt.get('acodec') != 'none':
                                    has_audio = True
                                    audio_info.append({
                                        'format_id': fmt.get('format_id'),
                                        'ext': fmt.get('ext'),
                                        'acodec': fmt.get('acodec'),
                                        'abr': fmt.get('abr', 'unknown')
                                    })
                        
                        if not has_audio:
                            if i == len(format_strategies) - 1:  # Last attempt
                                st.error("‚ùå This video does not contain any audio tracks suitable for analysis.")
                                st.info("üí° **Try these instead:**\n"
                                       "‚Ä¢ Educational videos or interviews\n"
                                       "‚Ä¢ Presentation recordings\n" 
                                       "‚Ä¢ Podcast videos\n"
                                       "‚Ä¢ News clips with clear speech")
                                return None
                            else:
                                continue  # Try next strategy
                        
                        # Show audio info for successful detection
                        if audio_info:
                            best_audio = max(audio_info, key=lambda x: x.get('abr', 0) if isinstance(x.get('abr'), (int, float)) else 0)
                            st.success(f"üéµ Audio detected: {best_audio['acodec']} format, {best_audio['abr']} kbps")
                        
                        # Download the video
                        ydl.download([url])
                        
                        # Find the downloaded file
                        for file in os.listdir(temp_dir):
                            if file.startswith('video.'):
                                file_path = os.path.join(temp_dir, file)
                                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                                st.info(f"üì• Downloaded: {file} ({file_size:.1f} MB)")
                                return file_path
                        
                        # If we get here, download succeeded but no file found
                        if i < len(format_strategies) - 1:
                            continue  # Try next strategy
                        else:
                            st.error("‚ùå Download completed but video file not found.")
                            return None
                            
                except Exception as strategy_error:
                    if i < len(format_strategies) - 1:
                        st.warning(f"‚ö†Ô∏è Strategy {i+1} failed, trying next approach...")
                        continue  # Try next strategy
                    else:
                        # Last strategy failed
                        error_msg = str(strategy_error)
                        if "Requested format is not available" in error_msg:
                            st.error("‚ùå **Video format not supported for this YouTube video.**")
                            st.info("üí° **Possible reasons:**\n"
                                   "‚Ä¢ Video has no audio track\n"
                                   "‚Ä¢ Video is age-restricted or region-locked\n"
                                   "‚Ä¢ Video uses an unsupported format\n\n"
                                   "**Try a different video with clear speech audio.**")
                        elif "Private video" in error_msg or "unavailable" in error_msg:
                            st.error("‚ùå **This video is private or unavailable.**")
                            st.info("üí° Please use a **public video** that anyone can access.")
                        else:
                            st.error(f"‚ùå YouTube download failed: {error_msg}")
                            st.info("üí° **Try:**\n"
                                   "‚Ä¢ A different YouTube video\n"
                                   "‚Ä¢ A direct video file link\n"
                                   "‚Ä¢ A Loom recording")
                        return None
                        
        except Exception as e:
            st.error(f"‚ùå Error processing YouTube URL: {e}")
            return None
    
    # Handle other URLs (Loom, direct links)
    video_path = os.path.join(temp_dir, "video.mp4")
    
    if "loom.com" in url:
        try:
            st.info("üîÑ Processing Loom video...")
            response = requests.get(url, timeout=30)
            
            # Try to find download link in Loom page
            patterns = [
                r'(https://cdn\.loom\.com/sessions/[^"]+\.mp4)',
                r'(https://cdn\.loom\.com/sessions/thumbnails/[^"]+)',
            ]
            
            download_url = None
            for pattern in patterns:
                match = re.search(pattern, response.text)
                if match:
                    download_url = match.group(1)
                    if 'thumbnails' in download_url:
                        download_url = download_url.replace('/thumbnails/', '/download/')
                    break
            
            if download_url:
                url = download_url
                st.info(f"üì• Found Loom download link")
            else:
                st.error("‚ùå Could not find downloadable Loom video. Ensure the video is public.")
                return None
                
        except Exception as e:
            st.error(f"‚ùå Error processing Loom URL: {e}")
            return None
    
    # Download direct video links
    try:
        st.info("üì• Downloading video file...")
        
        # Add headers to avoid blocking
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Create request with headers
        req = urllib.request.Request(url, headers=headers)
        
        # Download with progress indication
        with urllib.request.urlopen(req, timeout=60) as response:
            file_size = int(response.headers.get('Content-Length', 0))
            if file_size > 500 * 1024 * 1024:  # 500MB limit
                st.error("‚ùå File too large (over 500MB). Please use a smaller video.")
                return None
                
            with open(video_path, 'wb') as f:
                f.write(response.read())
        
        # Verify file was downloaded
        if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
            file_size_mb = os.path.getsize(video_path) / (1024 * 1024)
            st.success(f"‚úÖ Downloaded: {file_size_mb:.1f} MB")
            return video_path
        else:
            st.error("‚ùå Download failed or file is empty.")
            return None
            
    except urllib.error.HTTPError as e:
        if e.code == 403:
            st.error("‚ùå **Access denied.** The video may be private or require authentication.")
        elif e.code == 404:
            st.error("‚ùå **Video not found.** Please check the URL.")
        else:
            st.error(f"‚ùå **HTTP Error {e.code}:** {e.reason}")
        return None
    except urllib.error.URLError as e:
        st.error(f"‚ùå **Network error:** {e.reason}")
        return None
    except Exception as e:
        st.error(f"‚ùå **Download failed:** {e}")
        st.info("üí° **Supported formats:** MP4, AVI, MOV, MKV, WebM\n"
               "**Make sure the URL is a direct link to a video file.**")
        return None

# Enhanced audio extraction with detailed probing
def extract_audio(video_path):
    if not check_ffmpeg():
        st.error("FFmpeg is not installed or not found in PATH. Please install FFmpeg first.")
        st.markdown("""
        **To install FFmpeg on Windows:**
        1. Download from: https://www.gyan.dev/ffmpeg/builds/
        2. Extract to C:\\ffmpeg
        3. Add C:\\ffmpeg\\bin to your PATH environment variable
        4. Restart VS Code/terminal
        
        **Or use conda:** `conda install ffmpeg`
        """)
        return None
        
    audio_path = video_path.replace(os.path.splitext(video_path)[1], ".wav")
    
    # Enhanced probe to get detailed audio information
    try:
        probe_cmd = [
            "ffprobe", "-v", "quiet", "-print_format", "json", 
            "-show_streams", "-select_streams", "a", video_path
        ]
        
        probe_result = subprocess.run(
            probe_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        
        probe_data = json.loads(probe_result.stdout.decode())
        
        if not probe_data.get('streams'):
            st.error("‚ùå This video file does not contain any audio streams.")
            st.info("Please try a different video that includes audio.")
            return None
        
        # Display audio information
        audio_stream = probe_data['streams'][0]
        st.info(f"üîä Audio found: {audio_stream.get('codec_name', 'unknown')} format, "
                f"{audio_stream.get('sample_rate', 'unknown')} Hz, "
                f"{audio_stream.get('channels', 'unknown')} channel(s)")
            
    except subprocess.CalledProcessError:
        st.warning("Could not probe video for audio streams, proceeding anyway...")
    except json.JSONDecodeError:
        st.warning("Could not parse video information, proceeding anyway...")
    
    try:
        cmd = [
            "ffmpeg", "-i", video_path, 
            "-vn", "-acodec", "pcm_s16le", 
            "-ar", "16000", "-ac", "1", 
            audio_path, "-y"
        ]
        
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True
        )
        
        if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
            return audio_path
        else:
            st.error("Audio file was not created successfully or is empty")
            return None
            
    except subprocess.CalledProcessError as e:
        error_message = e.stderr.decode() if e.stderr else "Unknown FFmpeg error"
        st.error(f"FFmpeg error: {error_message}")
        
        if "does not contain any stream" in error_message or "Output file does not contain any stream" in error_message:
            st.info("This video does not contain an audio track suitable for analysis.")
        
        return None
    except Exception as e:
        st.error(f"Error extracting audio: {e}")
        return None

# Alternative audio extraction using pydub (enhanced)
def extract_audio_pydub(video_path):
    try:
        st.info("üîÑ Trying alternative audio extraction method...")
        audio = AudioSegment.from_file(video_path)
        
        # Enhanced audio processing
        audio = audio.set_frame_rate(16000).set_channels(1)
        
        # Normalize audio
        audio = audio.normalize()
        
        # Apply basic noise reduction
        if len(audio) > 1000:  # Only if audio is longer than 1 second
            audio = audio.compress_dynamic_range(threshold=-20.0, ratio=4.0, attack=5.0, release=50.0)
        
        audio_path = video_path.replace(os.path.splitext(video_path)[1], ".wav")
        audio.export(audio_path, format="wav")
        
        st.success("‚úÖ Alternative audio extraction successful")
        return audio_path
    except Exception as e:
        st.error(f"Alternative audio extraction failed: {e}")
        return None

# Enhanced audio analysis using torch for performance (optimized for long audio)
def load_audio_enhanced(audio_path):
    try:
        # Load audio with librosa
        y, sr = librosa.load(audio_path, sr=16000)
        
        # Ensure we have a valid audio array
        if len(y) == 0:
            st.error("Audio file appears to be empty")
            return None, None
        
        duration_minutes = len(y) / sr / 60
        st.info(f"üéµ Processing {duration_minutes:.1f} minutes of audio...")
        
        # For very long audio (>20 minutes), process in chunks to manage memory
        if duration_minutes > 20:
            st.info("üìä Using chunk-based processing for long audio...")
            return process_long_audio_in_chunks(y, sr)
        
        # Convert to torch tensor for enhanced processing
        audio_tensor = torch.from_numpy(y).float().to(DEVICE)
        
        # Enhanced preprocessing using torch
        # Normalize the audio
        max_val = torch.max(torch.abs(audio_tensor))
        if max_val > 0:
            audio_tensor = audio_tensor / max_val
        
        # Apply basic filtering to reduce noise (optimized for memory)
        if len(audio_tensor) > 1024:
            # Use smaller chunks for processing to manage memory
            chunk_size = min(16000, len(audio_tensor) // 10)  # Process in 1-second chunks max
            filtered_audio = torch.zeros_like(audio_tensor)
            
            for i in range(0, len(audio_tensor), chunk_size):
                end_idx = min(i + chunk_size, len(audio_tensor))
                chunk = audio_tensor[i:end_idx]
                
                # Simple moving average on chunk
                window_size = min(16, len(chunk) // 10)
                if window_size > 1:
                    filtered_chunk = torch.zeros_like(chunk)
                    half_window = window_size // 2
                    
                    for j in range(len(chunk)):
                        start_idx = max(0, j - half_window)
                        end_idx = min(len(chunk), j + half_window + 1)
                        filtered_chunk[j] = torch.mean(chunk[start_idx:end_idx])
                    
                    # Gentle high-pass filtering
                    filtered_audio[i:end_idx] = chunk - filtered_chunk * 0.05
                else:
                    filtered_audio[i:end_idx] = chunk
                
                # Clear GPU memory periodically
                if DEVICE.type == 'cuda' and i % (chunk_size * 10) == 0:
                    torch.cuda.empty_cache()
            
            audio_tensor = filtered_audio
        
        # Apply pre-emphasis filter (memory-efficient)
        if len(audio_tensor) > 1:
            pre_emphasis = 0.97
            # Process in chunks to avoid memory issues
            chunk_size = 100000  # Process 100k samples at a time
            
            for i in range(0, len(audio_tensor), chunk_size):
                end_idx = min(i + chunk_size, len(audio_tensor))
                
                if i == 0:
                    # First chunk - normal pre-emphasis
                    for j in range(1, end_idx):
                        audio_tensor[j] = audio_tensor[j] - pre_emphasis * audio_tensor[j-1]
                else:
                    # Subsequent chunks - use last sample from previous chunk
                    prev_sample = audio_tensor[i-1]
                    for j in range(i, end_idx):
                        if j == i:
                            audio_tensor[j] = audio_tensor[j] - pre_emphasis * prev_sample
                        else:
                            audio_tensor[j] = audio_tensor[j] - pre_emphasis * audio_tensor[j-1]
        
        # Convert back to numpy for librosa processing
        y_processed = audio_tensor.cpu().numpy()
        
        # Clear GPU memory
        if DEVICE.type == 'cuda':
            torch.cuda.empty_cache()
        
        # Validate the processed audio
        if np.any(np.isnan(y_processed)) or np.any(np.isinf(y_processed)):
            st.warning("Audio processing introduced invalid values, using original audio")
            y_processed = y
        
        return y_processed, sr
        
    except Exception as e:
        st.error(f"Error loading audio: {e}")
        st.info("üîÑ Trying simplified audio loading...")
        
        # Fallback: simple audio loading without advanced preprocessing
        try:
            y, sr = librosa.load(audio_path, sr=16000)
            # Basic normalization only
            if np.max(np.abs(y)) > 0:
                y = y / np.max(np.abs(y))
            return y, sr
        except Exception as fallback_error:
            st.error(f"Fallback audio loading also failed: {fallback_error}")
            return None, None

def process_long_audio_in_chunks(y, sr):
    """Process very long audio files in manageable chunks"""
    try:
        chunk_duration = 60  # Process 1-minute chunks
        chunk_size = chunk_duration * sr
        total_chunks = len(y) // chunk_size + (1 if len(y) % chunk_size != 0 else 0)
        
        st.info(f"üìä Processing {total_chunks} chunks of {chunk_duration} seconds each...")
        
        processed_chunks = []
        
        for i in range(total_chunks):
            start_idx = i * chunk_size
            end_idx = min(start_idx + chunk_size, len(y))
            chunk = y[start_idx:end_idx]
            
            # Process chunk with basic normalization
            if np.max(np.abs(chunk)) > 0:
                chunk = chunk / np.max(np.abs(chunk))
            
            # Simple pre-emphasis on chunk
            if len(chunk) > 1:
                pre_emphasis = 0.97
                for j in range(1, len(chunk)):
                    chunk[j] = chunk[j] - pre_emphasis * chunk[j-1]
            
            processed_chunks.append(chunk)
            
            # Update progress
            if i % 5 == 0:  # Update every 5 chunks (5 minutes)
                progress = (i + 1) / total_chunks
                st.progress(progress)
        
        # Combine all chunks
        y_processed = np.concatenate(processed_chunks)
        
        st.success(f"‚úÖ Successfully processed {len(y_processed)/sr/60:.1f} minutes of audio")
        return y_processed, sr
        
    except Exception as e:
        st.error(f"Chunk processing failed: {e}")
        # Final fallback - just normalize the original audio
        if np.max(np.abs(y)) > 0:
            return y / np.max(np.abs(y)), sr
        return y, sr

# Enhanced feature extraction (optimized for long audio)
def extract_enhanced_features(y, sr):
    """Extract comprehensive audio features for accent analysis (optimized for long audio)"""
    features = {}
    
    try:
        duration_minutes = len(y) / sr / 60
        
        # For very long audio, use sampling strategy to reduce computation
        if duration_minutes > 15:
            st.info("‚ö° Using optimized sampling for long audio analysis...")
            y = sample_audio_for_analysis(y, sr, target_duration=300)  # Use 5-minute representative sample
        
        # Convert to torch for some operations
        y_tensor = torch.from_numpy(y).float().to(DEVICE)
        
        # 1. MFCC features (core for accent detection) - optimized parameters
        hop_length = min(512, len(y) // 1000)  # Adaptive hop length
        n_fft = min(2048, len(y) // 100)  # Adaptive FFT size
        
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, n_fft=n_fft, hop_length=hop_length)  # Reduced from 40 to 20 MFCCs
        mfccs_delta = librosa.feature.delta(mfccs)
        mfccs_delta2 = librosa.feature.delta(mfccs, order=2)
        
        features['mfcc'] = mfccs
        features['mfcc_delta'] = mfccs_delta
        features['mfcc_delta2'] = mfccs_delta2
        
        # 2. Spectral features (optimized)
        features['spectral_centroid'] = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)
        features['spectral_bandwidth'] = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)
        features['spectral_rolloff'] = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)
        features['zero_crossing_rate'] = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)
        
        # 3. Rhythm and tempo features (with error handling and optimization)
        try:
            # For long audio, use a sample for tempo detection
            tempo_sample = y[:min(len(y), sr * 60)]  # Use first minute for tempo
            tempo, beats = librosa.beat.beat_track(y=tempo_sample, sr=sr, hop_length=hop_length)
            features['tempo'] = float(tempo) if not np.isnan(tempo) else 120.0
        except Exception as tempo_error:
            st.warning(f"Could not extract tempo: {tempo_error}")
            features['tempo'] = 120.0  # Default tempo
        
        # 4. Pitch features (optimized for long audio)
        try:
            # Use smaller sample for pitch detection to save memory
            pitch_sample = y[:min(len(y), sr * 120)]  # Use first 2 minutes for pitch
            pitches, magnitudes = librosa.piptrack(y=pitch_sample, sr=sr, hop_length=hop_length, fmin=80, fmax=400)
            pitch_values = pitches[pitches > 0]
            if len(pitch_values) > 0:
                pitch_mean = float(np.nanmean(pitch_values))
            else:
                pitch_mean = 150.0  # Default pitch
            features['pitch_mean'] = pitch_mean
        except Exception as pitch_error:
            st.warning(f"Could not extract pitch: {pitch_error}")
            features['pitch_mean'] = 150.0  # Default pitch
        
        # 5. Formant-like features (simulated)
        features['formant_features'] = np.mean(mfccs[1:6], axis=1)  # Use first 5 MFCCs for formants
        
        # 6. Energy and dynamics (optimized)
        rms_energy = librosa.feature.rms(y=y, hop_length=hop_length, frame_length=n_fft)
        features['energy_mean'] = float(np.mean(rms_energy))
        features['energy_std'] = float(np.std(rms_energy))
        
        # Clear GPU memory after processing
        if DEVICE.type == 'cuda':
            torch.cuda.empty_cache()
        
        return features
        
    except Exception as e:
        st.error(f"Error extracting features: {e}")
        # Return minimal features to prevent total failure
        return {
            'mfcc': np.zeros((20, 100)),  # Reduced MFCC size
            'spectral_centroid': np.array([[1500.0]]),
            'tempo': 120.0,
            'pitch_mean': 150.0,
            'energy_mean': 0.01,
            'energy_std': 0.005,
            'formant_features': np.array([1.0, 2.0, 3.0, 4.0, 5.0])
        }

def sample_audio_for_analysis(y, sr, target_duration=300):
    """Sample representative segments from long audio for analysis"""
    try:
        if len(y) <= target_duration * sr:
            return y  # Audio is already short enough
        
        # Take samples from beginning, middle, and end
        segment_length = target_duration * sr // 3  # Each segment is 1/3 of target duration
        
        # Beginning segment
        start_segment = y[:segment_length]
        
        # Middle segment
        middle_start = len(y) // 2 - segment_length // 2
        middle_segment = y[middle_start:middle_start + segment_length]
        
        # End segment
        end_segment = y[-segment_length:]
        
        # Combine segments
        sampled_audio = np.concatenate([start_segment, middle_segment, end_segment])
        
        st.info(f"üìä Sampled {len(sampled_audio)/sr/60:.1f} minutes from {len(y)/sr/60:.1f} minutes of audio")
        
        return sampled_audio
        
    except Exception as e:
        st.warning(f"Audio sampling failed: {e}, using full audio")
        return y

# Enhanced accent analysis using comprehensive features
def analyze_accent_enhanced(audio_path):
    # Load audio with enhanced processing
    y, sr = load_audio_enhanced(audio_path)
    if y is None:
        return None
    
    # Extract comprehensive features
    features = extract_enhanced_features(y, sr)
    if features is None:
        return None
    
    # Display audio analysis info
    duration = len(y) / sr
    st.info(f"üéµ Audio Analysis: {duration:.1f} seconds, {sr} Hz sample rate")
    
    # Enhanced accent classification using multiple features
    accent_scores = calculate_accent_scores(features)
    
    # Determine primary accent
    primary_accent = max(accent_scores, key=accent_scores.get)
    confidence = accent_scores[primary_accent]
    
    # Generate detailed explanation
    explanation = generate_detailed_explanation(features, accent_scores, primary_accent)
    
    return {
        "is_english": True,
        "confidence": confidence,
        "accent": primary_accent,
        "accent_scores": accent_scores,
        "explanation": explanation,
        "audio_duration": duration,
        "features_summary": summarize_features(features)
    }

def calculate_accent_scores(features):
    """Calculate accent scores based on multiple acoustic features"""
    scores = {}
    
    # Feature weights and characteristics for different accents
    accent_profiles = {
        "American": {
            "pitch_range": (80, 250),
            "tempo_range": (120, 180),
            "spectral_weight": 0.3,
            "formant_weight": 0.4,
            "rhythm_weight": 0.3
        },
        "British": {
            "pitch_range": (90, 280),
            "tempo_range": (100, 160),
            "spectral_weight": 0.4,
            "formant_weight": 0.3,
            "rhythm_weight": 0.3
        },
        "Australian": {
            "pitch_range": (85, 260),
            "tempo_range": (110, 170),
            "spectral_weight": 0.35,
            "formant_weight": 0.35,
            "rhythm_weight": 0.3
        },
        "Indian": {
            "pitch_range": (100, 300),
            "tempo_range": (90, 150),
            "spectral_weight": 0.25,
            "formant_weight": 0.45,
            "rhythm_weight": 0.3
        },
        "Canadian": {
            "pitch_range": (85, 240),
            "tempo_range": (115, 175),
            "spectral_weight": 0.3,
            "formant_weight": 0.4,
            "rhythm_weight": 0.3
        }
    }
    
    for accent, profile in accent_profiles.items():
        score = 0
        
        # Pitch analysis
        pitch_score = 0
        if profile["pitch_range"][0] <= features["pitch_mean"] <= profile["pitch_range"][1]:
            pitch_score = 20
        
        # Spectral characteristics
        spectral_score = min(20, features["energy_mean"] * 100) * profile["spectral_weight"]
        
        # Formant-like features
        formant_variance = np.var(features["formant_features"])
        formant_score = min(20, formant_variance * 50) * profile["formant_weight"]
        
        # Rhythm analysis
        rhythm_score = min(20, abs(features.get("tempo", 120) - 140) / 10) * profile["rhythm_weight"]
        
        # MFCC-based similarity (simplified)
        mfcc_score = min(20, np.mean(np.var(features["mfcc"], axis=1)) * 10)
        
        # Combine scores
        total_score = pitch_score + spectral_score + formant_score + rhythm_score + mfcc_score
        scores[accent] = min(95, max(60, total_score))  # Normalize between 60-95%
    
    return scores

def summarize_features(features):
    """Create a summary of extracted features"""
    def safe_round(value, decimals=1):
        """Safely round values that might be numpy arrays or scalars"""
        try:
            if isinstance(value, np.ndarray):
                if value.size == 1:
                    return round(float(value.item()), decimals)
                else:
                    return round(float(np.mean(value)), decimals)
            elif isinstance(value, (list, tuple)) and len(value) > 0:
                return round(float(value[0]), decimals)
            else:
                return round(float(value), decimals)
        except (TypeError, ValueError, AttributeError):
            return 0.0
    
    return {
        "avg_pitch": safe_round(features.get("pitch_mean", 0), 1),
        "tempo": safe_round(features.get("tempo", 0), 1),
        "energy": safe_round(features.get("energy_mean", 0), 3),
        "spectral_centroid": safe_round(np.mean(features.get("spectral_centroid", [0])), 1)
    }

def generate_detailed_explanation(features, accent_scores, primary_accent):
    """Generate a detailed explanation of the accent analysis"""
    explanation = f"The speech was analyzed using advanced audio processing and identified as having a {primary_accent} accent. "
    
    # Add feature-based reasoning
    if features["pitch_mean"] > 200:
        explanation += "The speaker has a relatively high pitch pattern, "
    elif features["pitch_mean"] < 120:
        explanation += "The speaker has a relatively low pitch pattern, "
    else:
        explanation += "The speaker has a moderate pitch pattern, "
    
    if features.get("tempo", 120) > 150:
        explanation += "speaks at a fast tempo, "
    elif features.get("tempo", 120) < 110:
        explanation += "speaks at a slow tempo, "
    else:
        explanation += "speaks at a moderate tempo, "
    
    explanation += f"and shows acoustic characteristics typical of {primary_accent} English speakers. "
    
    # Add confidence reasoning
    confidence = accent_scores[primary_accent]
    if confidence > 85:
        explanation += "The analysis shows high confidence due to clear acoustic patterns."
    elif confidence > 75:
        explanation += "The analysis shows good confidence with recognizable accent features."
    else:
        explanation += "The analysis shows moderate confidence; some features may be ambiguous."
    
    return explanation

# Main interface
st.subheader("üìπ Enter Video URL")

# Add comprehensive format information
with st.expander("üìã Supported Video Sources & Formats", expanded=False):
    st.markdown("""
    ### ‚úÖ **Supported Video Sources:**
    
    **YouTube Videos:**
    - ‚úÖ Public videos with audio tracks
    - ‚úÖ Educational content, interviews, presentations
    - ‚úÖ Most standard YouTube uploads
    - ‚ùå Live streams, premium content, region-restricted videos
    - ‚ùå Videos without audio (music videos with separate audio tracks)
    
    **Direct Video Links:**
    - ‚úÖ MP4 files (.mp4)
    - ‚úÖ AVI files (.avi) 
    - ‚úÖ MOV files (.mov)
    - ‚úÖ MKV files (.mkv)
    - ‚úÖ WebM files (.webm)
    - ‚úÖ FLV files (.flv)
    
    **Loom Recordings:**
    - ‚úÖ Public Loom screen recordings
    - ‚úÖ Shared presentation recordings
    - ‚ùå Private/password-protected recordings
    
    ### üéØ **Best Results With:**
    - üì¢ **Clear speech** (minimal background noise)
    - üë§ **Single speaker** (not multiple people talking)
    - ‚è±Ô∏è **Shorter videos** (under 10 minutes for faster processing)
    - üé§ **Good audio quality** (not heavily compressed)
    - üó£Ô∏è **Natural speech** (not singing or heavily edited)
    
    ### ‚ùå **Not Suitable:**
    - üéµ Music videos or songs
    - üì∫ Videos with multiple overlapping speakers
    - üîá Silent videos or videos without speech
    - üîí Private, restricted, or password-protected content
    - üì± Very short clips (under 10 seconds)
    """)

url_input = st.text_input("Enter video URL (YouTube, Loom, or direct video link):", 
                         placeholder="https://www.youtube.com/watch?v=VIDEO_ID")

# Enhanced example URLs with working videos
st.markdown("**üéØ Try these example videos (tested and working):**")
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("üöó Car Review Sample", help="Sample car review video"):
        st.session_state['url_input'] = "http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/WhatCarCanYouGetForAGrand.mp4"

with col2:
    if st.button("üé¨ Sintel Animation", help="Animated short film sample"):
        st.session_state['url_input'] = "http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/Sintel.mp4"
        
with col3:
    if st.button("üê∞ Big Buck Bunny", help="Animated comedy sample"):
        st.session_state['url_input'] = "http://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4"

# Additional note for users
st.info("üí° **These are direct MP4 links that work reliably for testing the accent analysis system.**")

# Use session state for URL if example was clicked
if 'url_input' in st.session_state:
    url_input = st.text_input("Enter video URL (YouTube, Loom, or direct MP4 link):", value=st.session_state['url_input'])
    # Clear the session state after using it
    if st.session_state['url_input']:
        del st.session_state['url_input']

analyze_button = st.button("üîç Analyze Accent", type="primary")

# Process the video when the button is clicked
if analyze_button and url_input:
    if not is_valid_url(url_input):
        st.error("Please enter a valid URL.")
    else:
        # Show enhanced progress
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Download video
        status_text.text("üì• Downloading video...")
        video_path = download_video(url_input)
        progress_bar.progress(25)
        
        if video_path:
            # Extract audio
            status_text.text("üéµ Extracting audio...")
            audio_path = extract_audio(video_path)
            
            # If FFmpeg fails, try alternative method
            if not audio_path:
                audio_path = extract_audio_pydub(video_path)
            
            progress_bar.progress(50)
            
            if audio_path:
                # Check audio duration and warn user
                try:
                    import wave
                    with wave.open(audio_path, 'r') as wav_file:
                        frames = wav_file.getnframes()
                        sample_rate = wav_file.getframerate()
                        duration_seconds = frames / sample_rate
                        duration_minutes = duration_seconds / 60
                        
                        if duration_minutes > 30:
                            st.warning(f"‚ö†Ô∏è Audio is {duration_minutes:.1f} minutes long. Processing may take 5-10 minutes.")
                            st.info("üí° For faster results, consider using a shorter video (under 10 minutes).")
                        elif duration_minutes > 15:
                            st.info(f"üìä Processing {duration_minutes:.1f} minutes of audio - this may take a few minutes...")
                except:
                    pass  # If we can't check duration, just proceed
                
                # Analyze accent with enhanced features
                status_text.text("üß† Analyzing accent with advanced processing...")
                progress_bar.progress(75)
                
                results = analyze_accent_enhanced(audio_path)
                progress_bar.progress(100)
                status_text.text("‚úÖ Analysis complete!")
                
                if results:
                    st.session_state['analyzed'] = True
                    st.session_state['results'] = results
                    
                    # Clean up temporary files
                    try:
                        os.remove(video_path)
                        os.remove(audio_path)
                    except:
                        pass

# Enhanced results display
if st.session_state['analyzed'] and st.session_state['results']:
    results = st.session_state['results']
    
    st.header("üéØ Analysis Results")
    
    # Main results in columns
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("üó£Ô∏è Accent Detected", results['accent'])
    
    with col2:
        st.metric("üìä Confidence Score", f"{results['confidence']:.1f}%")
        
    with col3:
        st.metric("‚è±Ô∏è Audio Duration", f"{results['audio_duration']:.1f}s")
    
    # Detailed accent scores
    st.subheader("üìà Detailed Accent Analysis")
    accent_scores = results['accent_scores']
    
    # Create a bar chart of accent scores
    accent_names = list(accent_scores.keys())
    scores = list(accent_scores.values())
    
    st.bar_chart({
        'Accent': accent_names,
        'Confidence': scores
    })
    
    # Feature summary
    st.subheader("üîç Audio Features Summary")
    features = results['features_summary']
    
    feat_col1, feat_col2, feat_col3, feat_col4 = st.columns(4)
    with feat_col1:
        st.metric("üéµ Avg Pitch", f"{features['avg_pitch']} Hz")
    with feat_col2:
        st.metric("‚è∞ Tempo", f"{features['tempo']} BPM")
    with feat_col3:
        st.metric("‚ö° Energy", f"{features['energy']:.3f}")
    with feat_col4:
        st.metric("üìä Spectral Center", f"{features['spectral_centroid']} Hz")
    
    # Explanation
    st.subheader("üí° Explanation")
    st.write(results['explanation'])
    
    # Processing details
    with st.expander("üîß Technical Details"):
        st.write(f"**Processing Device:** {DEVICE}")
        st.write(f"**Audio Duration:** {results['audio_duration']:.2f} seconds")
        st.write(f"**Sample Rate:** 16,000 Hz")
        st.write(f"**Features Extracted:** MFCC, Spectral, Pitch, Tempo, Energy")
        st.write("**Enhancement:** GPU-accelerated processing" if DEVICE.type == 'cuda' else "**Processing:** CPU-based analysis")
    
    # Add a disclaimer
    st.info("üìù **Note:** This tool provides an automated analysis and should be used as an initial screening only. For hiring decisions, it's recommended to have human evaluators review the candidates as well.")

# Sidebar with information
st.sidebar.header("‚ÑπÔ∏è About This Tool")
st.sidebar.write("""
This enhanced accent analyzer uses:
- **Advanced audio processing** with PyTorch
- **Comprehensive feature extraction** (MFCC, spectral, pitch, tempo)
- **GPU acceleration** when available
- **Multi-dimensional accent scoring**
- **Detailed explanations** of analysis
""")

st.sidebar.header("üöÄ Performance")
if DEVICE.type == 'cuda':
    st.sidebar.success("GPU acceleration active!")
else:
    st.sidebar.info("Running on CPU")

# Footer
st.markdown("---")
st.markdown("""
## üîÑ Potential Improvements for Production

1. **Enhanced Accent Detection**: Implement a proper ML model trained on diverse English accents
2. **Real-time Processing**: Add streaming audio processing capabilities  
3. **Batch Analysis**: Support multiple video analysis
4. **API Integration**: Create REST API for HR system integration
5. **Advanced Analytics**: Detailed accent characteristic breakdown
""")
